{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import h5py\n",
    "import time\n",
    "\n",
    "from einops import rearrange\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "from typing import Union, Optional, Callable\n",
    "from pathlib import Path\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mri_data_root = \"/data/projects/recon/data/public/qdess/v1-release/files_recon_calib-24/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SliceDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset that provides access to MR image slices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: Union[str, Path, os.PathLike],\n",
    "        transform: Optional[Callable] = None,\n",
    "        use_dataset_cache: bool = False,\n",
    "        sample_rate: Optional[float] = None,\n",
    "        volume_sample_rate: Optional[float] = None,\n",
    "        dataset_cache_file: Union[str, Path, os.PathLike] = \"dataset_cache.pkl\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root: Path to the dataset.\n",
    "            transform: Optional; A callable object that pre-processes the raw\n",
    "                data into appropriate form. The transform function should take\n",
    "                'kspace', 'target', 'attributes', 'filename', and 'slice' as\n",
    "                inputs. 'target' may be null for test data.\n",
    "            use_dataset_cache: Whether to cache dataset metadata. This is very\n",
    "                useful for large datasets like the brain data.\n",
    "            sample_rate: Optional; A float between 0 and 1. This controls what fraction\n",
    "                of the slices should be loaded. Defaults to 1 if no value is given.\n",
    "                When creating a sampled dataset either set sample_rate (sample by slices)\n",
    "                or volume_sample_rate (sample by volumes) but not both.\n",
    "            volume_sample_rate: Optional; A float between 0 and 1. This controls what fraction\n",
    "                of the volumes should be loaded. Defaults to 1 if no value is given.\n",
    "                When creating a sampled dataset either set sample_rate (sample by slices)\n",
    "                or volume_sample_rate (sample by volumes) but not both.\n",
    "            dataset_cache_file: Optional; A file in which to cache dataset\n",
    "                information for faster load times.\n",
    "        \"\"\"\n",
    "        if sample_rate is not None and volume_sample_rate is not None:\n",
    "            raise ValueError(\n",
    "                \"either set sample_rate (sample by slices) or volume_sample_rate (sample by volumes) but not both\"\n",
    "            )\n",
    "\n",
    "        self.dataset_cache_file = Path(dataset_cache_file)\n",
    "\n",
    "        self.transform = transform\n",
    "        self.examples = []\n",
    "\n",
    "        # set default sampling mode if none given\n",
    "        if sample_rate is None:\n",
    "            sample_rate = 1.0\n",
    "        if volume_sample_rate is None:\n",
    "            volume_sample_rate = 1.0\n",
    "\n",
    "        # load dataset cache if we have and user wants to use it\n",
    "        if self.dataset_cache_file.exists() and use_dataset_cache:\n",
    "            with open(self.dataset_cache_file, \"rb\") as f:\n",
    "                dataset_cache = pickle.load(f)\n",
    "        else:\n",
    "            dataset_cache = {}\n",
    "\n",
    "        # check if our dataset is in the cache\n",
    "        # if there, use that metadata, if not, then regenerate the metadata\n",
    "        if dataset_cache.get(root) is None or not use_dataset_cache:\n",
    "            files = list(Path(root).iterdir())\n",
    "            for fname in sorted(files):\n",
    "                num_slices = self._retrieve_metadata(fname)\n",
    "\n",
    "                self.examples += [\n",
    "                    (fname, slice_ind) for slice_ind in range(num_slices)\n",
    "                ]\n",
    "\n",
    "            if dataset_cache.get(root) is None and use_dataset_cache:\n",
    "                dataset_cache[root] = self.examples\n",
    "                logging.info(f\"Saving dataset cache to {self.dataset_cache_file}.\")\n",
    "                with open(self.dataset_cache_file, \"wb\") as f:\n",
    "                    pickle.dump(dataset_cache, f)\n",
    "        else:\n",
    "            logging.info(f\"Using dataset cache from {self.dataset_cache_file}.\")\n",
    "            self.examples = dataset_cache[root]\n",
    "\n",
    "        # subsample if desired\n",
    "        if sample_rate < 1.0:  # sample by slice\n",
    "            random.shuffle(self.examples)\n",
    "            num_examples = round(len(self.examples) * sample_rate)\n",
    "            self.examples = self.examples[:num_examples]\n",
    "        elif volume_sample_rate < 1.0:  # sample by volume\n",
    "            vol_names = sorted(list(set([f[0].stem for f in self.examples])))\n",
    "            random.shuffle(vol_names)\n",
    "            num_volumes = round(len(vol_names) * volume_sample_rate)\n",
    "            sampled_vols = vol_names[:num_volumes]\n",
    "            self.examples = [\n",
    "                example for example in self.examples if example[0].stem in sampled_vols\n",
    "            ]\n",
    "            \n",
    "    @staticmethod\n",
    "    def _retrieve_metadata(fname):\n",
    "        with h5py.File(fname, \"r\") as hf:\n",
    "            num_slices = hf[\"target\"].shape[2]\n",
    "\n",
    "        return num_slices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        fname, dataslice = self.examples[i]\n",
    "\n",
    "        with h5py.File(fname, \"r\") as hf:\n",
    "            target = hf[\"target\"][dataslice]\n",
    "            sample = self.transform(target, fname.name, dataslice)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransform:\n",
    "\n",
    "    def __init__(self, use_seed: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mask_func: Optional; A function that can create a mask of\n",
    "                appropriate shape. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.use_seed = use_seed\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        target: np.ndarray,\n",
    "        fname: str,\n",
    "        slice_num: int,\n",
    "    ) -> np.ndarray:\n",
    "        \n",
    "        return np.abs(rearrange(target, 'x y z e -> (z e) x y')), fname, slice_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_loaders(\n",
    "    data_path: Path,\n",
    "    sample_rate: float,\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    "    TrainingTransform: torchvision.transforms.Compose,\n",
    ") -> torch.utils.data.DataLoader:\n",
    "    train_loader = DataLoader(\n",
    "        dataset=SliceDataset(\n",
    "            root=data_path,\n",
    "            transform=TrainingTransform,\n",
    "            sample_rate=sample_rate,\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_training_loaders(mri_data_root, 1., 1, 4, DataTransform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed slices 24780. Time taken 20.35s\n"
     ]
    }
   ],
   "source": [
    "init_start = time.perf_counter()\n",
    "num_slices = 0\n",
    "for i, data in enumerate(train_loader):\n",
    "    (target, fname, slice_num) = data\n",
    "    num_slices = num_slices + 1\n",
    "print(f\"Parsed slices {num_slices}. Time taken {np.round(time.perf_counter() - init_start, 2)}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
